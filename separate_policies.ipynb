{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pykite as pk\n",
    "from utils import dql, dql_eval, plot_trajectory, dql_post\n",
    "from learning.deep.models import NN, NN5\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_attack=pk.coefficients.shape[0]\n",
    "n_bank=pk.bank_angles.shape[0]\n",
    "n_beta=pk.n_beta\n",
    "gamma=1\n",
    "eps0=0.01\n",
    "eta0=0.01\n",
    "episode_duration=300\n",
    "learning_step=0.2\n",
    "horizon=int(episode_duration/learning_step)\n",
    "integration_step=0.001\n",
    "integration_steps_per_learning_step=int(learning_step/integration_step)\n",
    "switch=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for dimension 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b6f9e6125dc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnet1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdurations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sgd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswitch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintegration_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintegration_steps_per_learning_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_decay_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps_decay_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meta0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/kitegen/utils.py\u001b[0m in \u001b[0;36mdql\u001b[0;34m(net, opt, episodes, horizon, learning_step, integration_step, integration_steps_per_learning_step, initial_pos, initial_vel, wind, lr, lr_decay_exp, lr_decay_start, eps, eps_decay_exp, eps_decay_start, gamma, plot)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdql_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_vel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhorizon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintegration_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintegration_steps_per_learning_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_decay_exp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_decay_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps_decay_exp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps_decay_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdurations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdurations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/kitegen/utils.py\u001b[0m in \u001b[0;36mdql_episode\u001b[0;34m(net, optimizer, loss, initial_position, initial_velocity, wind, horizon, learning_step, integration_step, integration_steps_per_learning_step, eta0, eta_decay, eta_decay_start, eps0, eps_decay, eps_decay_start, durations, rewards, gamma, t, plot)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mA_t\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0meps\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mA_t\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mA_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mA_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;31m#print(q)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m#print(A_t)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for dimension 0 with size 0"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "net1=NN()\n",
    "durations, rewards,_,_,_=dql(net1, 'sgd', 1000, switch, learning_step, integration_step, integration_steps_per_learning_step, pk.vect(np.pi/6, 0, 50), pk.vect(0, 0, 0), 10, lr_decay_start=50000, eps_decay_start=100000, eps=eps0, lr=eta0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  140  reward  -2.294306634714818\n",
      "1\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  114  reward  -2.4558664058609576\n",
      "2\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  133  reward  -2.3571634033131987\n",
      "3\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  169  reward  -2.1064204593453035\n",
      "4\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  151  reward  -2.258208251240802\n",
      "5\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  139  reward  -2.360246111454326\n",
      "6\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  281  reward  -1.0691413297385026\n",
      "7\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  157  reward  -2.1393258214774464\n",
      "8\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  150  reward  -2.176369220700832\n",
      "9\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  143  reward  -2.2067319021044534\n",
      "10\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  554  reward  3.566422575847688\n",
      "11\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  202  reward  -1.6501664925734076\n",
      "12\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  125  reward  -2.4676454035032176\n",
      "13\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  125  reward  -2.43490033656992\n",
      "14\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  138  reward  -2.233122527878105\n",
      "15\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  159  reward  -2.344667351248506\n",
      "16\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  127  reward  -2.3627083493437397\n",
      "17\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  122  reward  -2.404631736077322\n",
      "18\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  123  reward  -2.3391623275437277\n",
      "19\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  123  reward  -2.440772317917957\n",
      "20\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  124  reward  -2.403803078466437\n",
      "21\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  129  reward  -2.4191613801822935\n",
      "22\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  161  reward  -2.3559894297448176\n",
      "23\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  124  reward  -2.43542479141528\n",
      "24\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  215  reward  -2.30669253741413\n",
      "25\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  159  reward  -1.950479284506034\n",
      "26\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  143  reward  -2.3578757486231177\n",
      "27\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  325  reward  -1.378640376429679\n",
      "28\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  151  reward  -2.334607909006\n",
      "29\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  529  reward  3.9050118250217483\n",
      "30\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  220  reward  -1.664347181543091\n",
      "31\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  217  reward  -2.0234036615909528\n",
      "32\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  887  reward  4.706524094885073\n",
      "33\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  175  reward  -2.240797510732741\n",
      "34\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  135  reward  -2.404555550954399\n",
      "35\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  457  reward  2.2780556630682853\n",
      "36\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  153  reward  -2.396211557483751\n",
      "37\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  182  reward  -2.3641812743664787\n",
      "38\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  130  reward  -2.3960108429115\n",
      "39\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  214  reward  -2.0033940091046154\n",
      "40\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  194  reward  -1.9881642097326873\n",
      "41\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  171  reward  -2.1425538043800785\n",
      "42\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  511  reward  4.080432213129354\n",
      "43\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  158  reward  -2.3954384321421527\n",
      "44\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  136  reward  -2.4499684271660542\n",
      "45\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  134  reward  -2.450844867013185\n",
      "46\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  141  reward  -2.3133601549894216\n",
      "47\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  133  reward  -2.28012201567853\n",
      "48\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  138  reward  -2.3021644560751247\n",
      "49\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  188  reward  -2.1241758040990932\n",
      "50\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  127  reward  -2.448218105275298\n",
      "51\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  248  reward  -1.7701209149541408\n",
      "52\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  151  reward  -2.327730645071676\n",
      "53\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  157  reward  -2.155912222072004\n",
      "54\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  175  reward  -2.1536335129056687\n",
      "55\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  156  reward  -2.3216273141356116\n",
      "56\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  126  reward  -2.442587641456212\n",
      "57\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  215  reward  -1.6936189078565063\n",
      "58\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  144  reward  -2.3716925451880706\n",
      "59\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  161  reward  -2.1188696080911784\n",
      "60\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  137  reward  -2.3176948254335907\n",
      "61\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  140  reward  -2.36085467693833\n",
      "62\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  143  reward  -2.3150090272421724\n",
      "63\n",
      "Simulation ended at learning step:  1499  reward  16.352367019306737\n",
      "64\n",
      "Simulation ended at learning step:  1499  reward  23.714429154732574\n",
      "65\n",
      "Simulation ended at learning step:  1499  reward  16.45711963055716\n",
      "66\n",
      "Simulation ended at learning step:  1499  reward  22.27578482781399\n",
      "67\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  1139  reward  13.282575591353819\n",
      "68\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  362  reward  -1.4658702416773177\n",
      "69\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  157  reward  -2.2925887446355255\n",
      "70\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  142  reward  -2.3728031824266917\n",
      "71\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  123  reward  -2.433854160015431\n",
      "72\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  218  reward  -2.0993102697267663\n",
      "73\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  130  reward  -2.397027667106366\n",
      "74\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  128  reward  -2.431448518573902\n",
      "75\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  820  reward  61.03397451173132\n",
      "76\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  128  reward  -2.4140859453603256\n",
      "77\n",
      "Simulation ended at learning step:  1499  reward  119.5818569382274\n",
      "78\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  854  reward  2.74646509978807\n",
      "79\n",
      "Simulation ended at learning step:  1499  reward  15.618878243736171\n",
      "80\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  172  reward  -2.24439645211611\n",
      "81\n",
      "Simulation ended at learning step:  1499  reward  47.17436589528103\n",
      "82\n",
      "Simulation ended at learning step:  1499  reward  31.670429600681885\n",
      "83\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  638  reward  2.2305331770436396\n",
      "84\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  921  reward  5.336472217400511\n",
      "85\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  347  reward  -1.7341743342788045\n",
      "86\n",
      "Simulation ended at learning step:  1499  reward  24.340185006968063\n",
      "87\n",
      "Simulation ended at learning step:  1499  reward  7.347824743116795\n",
      "88\n",
      "Simulation ended at learning step:  1499  reward  17.592119212222\n",
      "89\n",
      "Simulation ended at learning step:  1499  reward  31.85839048884589\n",
      "90\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  171  reward  -2.2616786051287274\n",
      "91\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  751  reward  2.433268879232982\n",
      "92\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  792  reward  2.393893789872891\n",
      "93\n",
      "Simulation ended at learning step:  1499  reward  8.892831927910263\n",
      "94\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  587  reward  1.7098664837663897\n",
      "95\n",
      "Simulation ended at learning step:  1499  reward  7.456340457957743\n",
      "96\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  1117  reward  26.424001320564113\n",
      "97\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  203  reward  -2.1274444901136436\n",
      "98\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  125  reward  -2.479019967560849\n",
      "99\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  150  reward  -2.314857380444389\n",
      "100\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  150  reward  -2.251424235238899\n",
      "101\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  228  reward  -2.0669213001798927\n",
      "102\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  177  reward  -2.040316084511579\n",
      "103\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  127  reward  -2.421632325284897\n",
      "104\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  127  reward  -2.4286823864125933\n",
      "105\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  304  reward  -1.7577669494018398\n",
      "106\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  330  reward  -1.579902341707916\n",
      "107\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  198  reward  -2.0670271559634736\n",
      "108\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  419  reward  1.4215049051420647\n",
      "109\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  1117  reward  4.882523108546327\n",
      "110\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  695  reward  2.463384263039565\n",
      "111\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  195  reward  -2.2794076078681433\n",
      "112\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  138  reward  -2.347378521745349\n",
      "113\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  131  reward  -2.429847526661175\n",
      "114\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  123  reward  -2.4263292237922474\n",
      "115\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  120  reward  -2.4633914447234035\n",
      "116\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  119  reward  -2.4904299424276393\n",
      "117\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  116  reward  -2.4284055869125374\n",
      "118\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  117  reward  -2.422884718798384\n",
      "119\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  119  reward  -2.4168326667160893\n",
      "120\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  130  reward  -2.4671221678416857\n",
      "121\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  142  reward  -2.357439873759016\n",
      "122\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  253  reward  -2.0135619911850915\n",
      "123\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  239  reward  -1.9766317485517089\n",
      "124\n",
      "epsilon  0.05  eta 0.01 Simulation failed at learning step:  141  reward  -2.351715492238459\n",
      "125\n",
      "Simulation ended at learning step:  1499  reward  14.315209397901159\n",
      "126\n",
      "Simulation ended at learning step:  1499  reward  5.85919513656234\n",
      "127\n",
      "Simulation ended at learning step:  1499  reward  8.78640129687216\n",
      "128\n",
      "Simulation ended at learning step:  1499  reward  5.365568926682151\n",
      "129\n",
      "Simulation ended at learning step:  1499  reward  8.066289932960657\n",
      "130\n",
      "Simulation ended at learning step:  1499  reward  17.83118512323183\n",
      "131\n",
      "Simulation ended at learning step:  1499  reward  6.292366717506844\n",
      "132\n",
      "Simulation ended at learning step:  1499  reward  17.71331821691261\n",
      "133\n",
      "Simulation ended at learning step:  1499  reward  15.561525401303301\n",
      "134\n",
      "Simulation ended at learning step:  1499  reward  10.841814414591914\n",
      "135\n",
      "Simulation ended at learning step:  1499  reward  14.119384570438925\n",
      "136\n",
      "Simulation ended at learning step:  1499  reward  19.598198832803785\n",
      "137\n",
      "Simulation ended at learning step:  1499  reward  18.245384936875247\n",
      "138\n",
      "Simulation ended at learning step:  1499  reward  19.723306612255733\n",
      "139\n",
      "epsilon  0.05  eta 2.5416864252065643e-05 Simulation failed at learning step:  694  reward  2.0838167363727855\n",
      "140\n",
      "Simulation ended at learning step:  1499  reward  19.376742443529565\n",
      "141\n",
      "Simulation ended at learning step:  1499  reward  24.25225853724893\n",
      "142\n",
      "Simulation ended at learning step:  1499  reward  22.26323320694372\n",
      "143\n",
      "Simulation ended at learning step:  1499  reward  16.705693107227525\n",
      "144\n",
      "Simulation ended at learning step:  1499  reward  8.27712420771631\n",
      "145\n",
      "Simulation ended at learning step:  1499  reward  22.75193137596678\n",
      "146\n",
      "epsilon  0.05  eta 2.0056607374742758e-05 Simulation failed at learning step:  1224  reward  3.746627824343382\n",
      "147\n",
      "Simulation ended at learning step:  1499  reward  17.249029869392984\n",
      "148\n",
      "Simulation ended at learning step:  1499  reward  13.963024815713284\n",
      "149\n",
      "Simulation ended at learning step:  1499  reward  23.014045560774246\n",
      "150\n",
      "Simulation ended at learning step:  1499  reward  16.6937759780673\n",
      "151\n",
      "Simulation ended at learning step:  1499  reward  7.938184746695281\n",
      "152\n",
      "Simulation ended at learning step:  1499  reward  15.236324287875878\n",
      "153\n",
      "epsilon  0.05  eta 1.6987907979209152e-05 Simulation failed at learning step:  995  reward  3.00836149259663\n",
      "154\n",
      "Simulation ended at learning step:  1499  reward  17.51155429480155\n",
      "155\n",
      "Simulation ended at learning step:  1499  reward  13.065154721876853\n",
      "156\n",
      "Simulation ended at learning step:  1499  reward  6.842829231579025\n",
      "157\n",
      "Simulation ended at learning step:  1499  reward  5.499456811641595\n",
      "158\n",
      "Simulation ended at learning step:  1499  reward  16.122434714771227\n",
      "159\n",
      "Simulation ended at learning step:  1499  reward  12.904326973313934\n",
      "160\n",
      "Simulation ended at learning step:  1499  reward  5.561562723437092\n",
      "161\n",
      "Simulation ended at learning step:  1499  reward  6.132569430224073\n",
      "162\n",
      "Simulation ended at learning step:  1499  reward  19.497006679332\n",
      "163\n",
      "Simulation ended at learning step:  1499  reward  8.004827414039713\n",
      "164\n",
      "Simulation ended at learning step:  1499  reward  6.565927496642976\n",
      "165\n",
      "Simulation ended at learning step:  1499  reward  5.358522570122002\n",
      "166\n",
      "Simulation ended at learning step:  1499  reward  20.78891624206109\n",
      "167\n",
      "Simulation ended at learning step:  1499  reward  6.476944718248479\n",
      "168\n",
      "Simulation ended at learning step:  1499  reward  8.610069772880584\n",
      "169\n",
      "Simulation ended at learning step:  1499  reward  5.114646702298716\n",
      "170\n",
      "Simulation ended at learning step:  1499  reward  22.278513784116456\n",
      "171\n",
      "Simulation ended at learning step:  1499  reward  12.997622469286123\n",
      "172\n",
      "Simulation ended at learning step:  1499  reward  14.936771359434532\n",
      "173\n",
      "Simulation ended at learning step:  1499  reward  20.05561986425881\n",
      "174\n",
      "Simulation ended at learning step:  1499  reward  7.576409339688748\n",
      "175\n",
      "Simulation ended at learning step:  1499  reward  13.7590513847204\n",
      "176\n",
      "Simulation ended at learning step:  1499  reward  5.111424469015143\n",
      "177\n",
      "Simulation ended at learning step:  1499  reward  4.684173102968392\n",
      "178\n",
      "Simulation ended at learning step:  1499  reward  5.870037516727395\n",
      "179\n",
      "Simulation ended at learning step:  1499  reward  6.4127699791573205\n",
      "180\n",
      "Simulation ended at learning step:  1499  reward  6.2830393076794575\n",
      "181\n",
      "Simulation ended at learning step:  1499  reward  6.410276765261175\n",
      "182\n",
      "Simulation ended at learning step:  1499  reward  7.360073585203045\n",
      "183\n",
      "Simulation ended at learning step:  1499  reward  7.287940620054562\n",
      "184\n",
      "Simulation ended at learning step:  1499  reward  7.580818988517972\n",
      "185\n",
      "Simulation ended at learning step:  1499  reward  16.7141665473147\n",
      "186\n",
      "Simulation ended at learning step:  1499  reward  6.624186375348845\n",
      "187\n",
      "Simulation ended at learning step:  1499  reward  6.796978347365447\n",
      "188\n",
      "Simulation ended at learning step:  1499  reward  6.773255761084956\n",
      "189\n",
      "Simulation ended at learning step:  1499  reward  7.515693714362681\n",
      "190\n",
      "Simulation ended at learning step:  1499  reward  5.470506200131128\n",
      "191\n",
      "Simulation ended at learning step:  1499  reward  16.63449678307591\n",
      "192\n",
      "Simulation ended at learning step:  1499  reward  5.152475979354063\n",
      "193\n",
      "Simulation ended at learning step:  1499  reward  5.174972832990668\n",
      "194\n",
      "Simulation ended at learning step:  1499  reward  6.0104506745597\n",
      "195\n",
      "Simulation ended at learning step:  1499  reward  5.799033654581087\n",
      "196\n",
      "Simulation ended at learning step:  1499  reward  16.188781792227182\n",
      "197\n",
      "Simulation ended at learning step:  1499  reward  6.927419490277952\n",
      "198\n",
      "Simulation ended at learning step:  1499  reward  5.450733203682367\n",
      "199\n",
      "Simulation ended at learning step:  1499  reward  5.441856000265555\n",
      "200\n",
      "Simulation ended at learning step:  1499  reward  5.92876300707358\n",
      "201\n",
      "Simulation ended at learning step:  1499  reward  5.116226064515886\n",
      "202\n",
      "Simulation ended at learning step:  1499  reward  5.92876300707358\n",
      "203\n",
      "Simulation ended at learning step:  1499  reward  15.627153845507149\n",
      "204\n",
      "Simulation ended at learning step:  1499  reward  5.180559237462945\n",
      "205\n",
      "Simulation ended at learning step:  1499  reward  5.479427018483278\n",
      "206\n",
      "Simulation ended at learning step:  1499  reward  5.956118974507668\n",
      "207\n",
      "Simulation ended at learning step:  1499  reward  8.897158434691377\n",
      "208\n",
      "Simulation ended at learning step:  1499  reward  5.907313195770274\n",
      "209\n",
      "Simulation ended at learning step:  1499  reward  17.096774144925657\n",
      "210\n",
      "Simulation ended at learning step:  1499  reward  10.697925905552783\n",
      "211\n",
      "Simulation ended at learning step:  1499  reward  16.51334858045577\n",
      "212\n",
      "Simulation ended at learning step:  1499  reward  7.383167074743977\n",
      "213\n",
      "Simulation ended at learning step:  1499  reward  5.91699233914732\n",
      "214\n",
      "Simulation ended at learning step:  1499  reward  5.447667606930515\n",
      "215\n",
      "Simulation ended at learning step:  1499  reward  16.392447209180673\n",
      "216\n",
      "Simulation ended at learning step:  1499  reward  5.131096220792421\n",
      "217\n",
      "Simulation ended at learning step:  1499  reward  7.407450504096545\n",
      "218\n",
      "Simulation ended at learning step:  1499  reward  5.645712396982087\n",
      "219\n",
      "Simulation ended at learning step:  1499  reward  7.362244005454136\n",
      "220\n",
      "Simulation ended at learning step:  1499  reward  5.999209761663767\n",
      "221\n",
      "Simulation ended at learning step:  1499  reward  5.967925222922542\n",
      "222\n",
      "Simulation ended at learning step:  1499  reward  13.344219777748842\n",
      "223\n",
      "Simulation ended at learning step:  1499  reward  17.350820307872826\n",
      "224\n",
      "Simulation ended at learning step:  1499  reward  5.962743119247864\n",
      "225\n",
      "Simulation ended at learning step:  1499  reward  17.522838092473517\n",
      "226\n",
      "Simulation ended at learning step:  1499  reward  6.985666314318654\n",
      "227\n",
      "Simulation ended at learning step:  1499  reward  5.936780736776285\n",
      "228\n",
      "Simulation ended at learning step:  1499  reward  6.005351869360999\n",
      "229\n",
      "Simulation ended at learning step:  1499  reward  5.4941052774897745\n",
      "230\n",
      "Simulation ended at learning step:  1499  reward  7.330854644894227\n",
      "231\n",
      "Simulation ended at learning step:  1499  reward  5.460489822590459\n",
      "232\n",
      "Simulation ended at learning step:  1499  reward  11.771209702155344\n",
      "233\n",
      "Simulation ended at learning step:  1499  reward  7.4437496373114405\n",
      "234\n",
      "Simulation ended at learning step:  1499  reward  12.16851837890461\n",
      "235\n",
      "Simulation ended at learning step:  1499  reward  5.898753766482114\n",
      "236\n",
      "Simulation ended at learning step:  1499  reward  13.689273019442432\n",
      "237\n",
      "Simulation ended at learning step:  1499  reward  7.350494514287609\n",
      "238\n",
      "Simulation ended at learning step:  1499  reward  7.459855446242057\n",
      "239\n",
      "Simulation ended at learning step:  1499  reward  6.4699964543077675\n",
      "240\n",
      "Simulation ended at learning step:  1499  reward  6.044210813545986\n",
      "241\n",
      "Simulation ended at learning step:  1499  reward  5.900775617980213\n",
      "242\n",
      "Simulation ended at learning step:  1499  reward  5.434479490052376\n",
      "243\n",
      "Simulation ended at learning step:  1499  reward  6.933462126542924\n",
      "244\n",
      "Simulation ended at learning step:  1499  reward  5.441092369078541\n",
      "245\n",
      "Simulation ended at learning step:  1499  reward  12.230403978624942\n",
      "246\n",
      "Simulation ended at learning step:  1499  reward  6.0608675686729026\n",
      "247\n",
      "Simulation ended at learning step:  1499  reward  5.919051188922455\n",
      "248\n",
      "Simulation ended at learning step:  1499  reward  12.6200873243643\n",
      "249\n",
      "Simulation ended at learning step:  1499  reward  5.436709099422631\n",
      "250\n",
      "Simulation ended at learning step:  1499  reward  7.43984012255312\n",
      "251\n",
      "Simulation ended at learning step:  1499  reward  5.5052778918070695\n",
      "252\n",
      "Simulation ended at learning step:  1499  reward  13.582477063802777\n",
      "253\n",
      "Simulation ended at learning step:  1499  reward  7.423503752030104\n",
      "254\n",
      "Simulation ended at learning step:  1499  reward  5.923801787919218\n",
      "255\n",
      "Simulation ended at learning step:  1499  reward  16.45809434758177\n",
      "256\n",
      "Simulation ended at learning step:  1499  reward  5.429962929453176\n",
      "257\n",
      "Simulation ended at learning step:  1499  reward  17.34830318908376\n",
      "258\n",
      "Simulation ended at learning step:  1499  reward  7.444174794572617\n",
      "259\n",
      "Simulation ended at learning step:  1499  reward  7.392135568000802\n",
      "260\n",
      "Simulation ended at learning step:  1499  reward  7.422243903651045\n",
      "261\n",
      "Simulation ended at learning step:  1499  reward  13.232913999250949\n",
      "262\n",
      "Simulation ended at learning step:  1499  reward  13.391728857586758\n",
      "263\n",
      "Simulation ended at learning step:  1499  reward  5.18706988193887\n",
      "264\n",
      "Simulation ended at learning step:  1499  reward  6.083470999219434\n",
      "265\n",
      "Simulation ended at learning step:  1499  reward  5.973207440438362\n",
      "266\n",
      "Simulation ended at learning step:  1499  reward  5.934177191697161\n",
      "267\n",
      "Simulation ended at learning step:  1499  reward  5.111512922183417\n",
      "268\n",
      "Simulation ended at learning step:  1499  reward  6.007143296524278\n",
      "269\n",
      "Simulation ended at learning step:  1499  reward  7.338309456642937\n",
      "270\n",
      "Simulation ended at learning step:  1499  reward  7.324987375975362\n",
      "271\n",
      "Simulation ended at learning step:  1499  reward  5.428984705989475\n",
      "272\n",
      "Simulation ended at learning step:  1499  reward  5.910026610705896\n",
      "273\n",
      "Simulation ended at learning step:  1499  reward  13.50865034373694\n",
      "274\n",
      "Simulation ended at learning step:  1499  reward  7.373725873125093\n",
      "275\n",
      "Simulation ended at learning step:  1499  reward  5.4622008080762665\n",
      "276\n",
      "Simulation ended at learning step:  1499  reward  5.988963673667106\n",
      "277\n",
      "Simulation ended at learning step:  1499  reward  13.418316567568727\n",
      "278\n",
      "Simulation ended at learning step:  1499  reward  7.3618564783715055\n",
      "279\n",
      "Simulation ended at learning step:  1499  reward  5.196068430641961\n",
      "280\n",
      "Simulation ended at learning step:  1499  reward  7.41722778598986\n",
      "281\n",
      "Simulation ended at learning step:  1499  reward  7.370538546153638\n",
      "282\n",
      "Simulation ended at learning step:  1499  reward  5.111367554407179\n",
      "283\n",
      "Simulation ended at learning step:  1499  reward  7.4917253799120385\n",
      "284\n",
      "Simulation ended at learning step:  1499  reward  6.021248089781575\n",
      "285\n",
      "Simulation ended at learning step:  1499  reward  7.418191508081512\n",
      "286\n",
      "Simulation ended at learning step:  1499  reward  8.585819026238253\n",
      "287\n",
      "Simulation ended at learning step:  1499  reward  6.547650347777963\n",
      "288\n",
      "Simulation ended at learning step:  1499  reward  5.956225517010113\n",
      "289\n",
      "Simulation ended at learning step:  1499  reward  8.585819026238253\n",
      "290\n",
      "Simulation ended at learning step:  1499  reward  5.991587479308522\n",
      "291\n",
      "Simulation ended at learning step:  1499  reward  6.023360687011878\n",
      "292\n",
      "Simulation ended at learning step:  1499  reward  4.987822882497088\n",
      "293\n",
      "Simulation ended at learning step:  1499  reward  5.509109192234044\n",
      "294\n",
      "Simulation ended at learning step:  1499  reward  7.46726226867088\n",
      "295\n",
      "Simulation ended at learning step:  1499  reward  5.932052729609732\n",
      "296\n",
      "Simulation ended at learning step:  1499  reward  5.120619865998432\n",
      "297\n",
      "Simulation ended at learning step:  1499  reward  7.403103357413659\n",
      "298\n",
      "Simulation ended at learning step:  1499  reward  13.569495956998455\n",
      "299\n",
      "Simulation ended at learning step:  1499  reward  6.007810481379848\n",
      "300\n",
      "Simulation ended at learning step:  1499  reward  7.49622422244911\n",
      "301\n",
      "Simulation ended at learning step:  1499  reward  5.101089979144455\n",
      "302\n",
      "Simulation ended at learning step:  1499  reward  6.015565703618747\n",
      "303\n",
      "Simulation ended at learning step:  1499  reward  6.650465376716018\n",
      "304\n",
      "Simulation ended at learning step:  1499  reward  6.66472862402231\n",
      "305\n",
      "Simulation ended at learning step:  1499  reward  5.463060597262593\n",
      "306\n",
      "Simulation ended at learning step:  1499  reward  6.731218699024579\n",
      "307\n",
      "Simulation ended at learning step:  1499  reward  5.942282951808947\n",
      "308\n",
      "Simulation ended at learning step:  1499  reward  6.028307053042712\n",
      "309\n",
      "Simulation ended at learning step:  1499  reward  5.487366020443068\n",
      "310\n",
      "Simulation ended at learning step:  1499  reward  7.469452922322956\n",
      "311\n",
      "Simulation ended at learning step:  1499  reward  11.153019744709994\n",
      "312\n",
      "Simulation ended at learning step:  1499  reward  13.376899045175456\n",
      "313\n",
      "Simulation ended at learning step:  1499  reward  5.179204107970559\n",
      "314\n",
      "Simulation ended at learning step:  1499  reward  8.608570610847185\n",
      "315\n",
      "Simulation ended at learning step:  1499  reward  4.71398822752398\n",
      "316\n",
      "Simulation ended at learning step:  1499  reward  6.025834159897315\n",
      "317\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-90a7e03d31e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnet2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdurations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdql_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswitch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sgd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhorizon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintegration_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintegration_steps_per_learning_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_decay_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps_decay_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meps0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meta0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/kitegen/utils.py\u001b[0m in \u001b[0;36mdql_post\u001b[0;34m(fixed_net, switch, net, opt, episodes, horizon, learning_step, integration_step, integration_steps_per_learning_step, initial_pos, initial_vel, wind, lr, lr_decay_exp, lr_decay_start, eps, eps_decay_exp, eps_decay_start, gamma, plot)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdql_post_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfixed_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswitch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_vel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhorizon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintegration_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintegration_steps_per_learning_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_decay_exp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_decay_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps_decay_exp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps_decay_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdurations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdurations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/kitegen/utils.py\u001b[0m in \u001b[0;36mdql_post_episode\u001b[0;34m(fixed_net, switch, net, optimizer, loss, initial_position, initial_velocity, wind, horizon, learning_step, integration_step, integration_steps_per_learning_step, eta0, eta_decay, eta_decay_start, eps0, eps_decay, eps_decay_start, durations, rewards, gamma, t, plot)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/profiler.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_callbacks_on_exit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_function_exit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_end_callbacks_on_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFuture\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mFuture\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net2=NN()\n",
    "durations, rewards,_,_,_=dql_post(net1, switch, net2, 'sgd', 1000, horizon, learning_step, integration_step, integration_steps_per_learning_step, pk.vect(np.pi/6, 0, 50), pk.vect(0, 0, 0), 10, lr_decay_start=50000, eps_decay_start=100000, eps=5*eps0, lr=eta0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
